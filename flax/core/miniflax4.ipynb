{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "from typing import Any, Callable, Iterable, List, Optional, Tuple, Type\n",
    "from flax import nn\n",
    "from flax.nn import initializers\n",
    "from flax.core.scope import Scope\n",
    "# CONSIDER: Rename scope to scoping? I really like being able to name variables `scope`\n",
    "from flax.core import scope\n",
    "from jax import numpy as jnp\n",
    "import functools\n",
    "\n",
    "from flax.core.frozen_dict import freeze\n",
    "\n",
    "@dataclass\n",
    "# TODO: Document that any class that extends from Module must add\n",
    "#   name = Optional[None]\n",
    "class Module:\n",
    "  parent: Optional[Type[\"Module\"]]\n",
    "  \n",
    "  # TODO: Use Dataclass \"hidden\" attributes that don't appear on __init__.\n",
    "  # Then remove all use of hasattr\n",
    "  \n",
    "  @classmethod\n",
    "  def toplevel(cls, *args, rngs=None, variables=None, mutable=False, **kwargs):\n",
    "    # TODO: Think about the fact that `rngs` and `params` live on args\n",
    "    # and kwargs\n",
    "    if rngs is None:\n",
    "      rngs = {}\n",
    "    if variables is None:\n",
    "      variables = {'param': {}}\n",
    "    module = cls(Scope(variables, rngs=rngs), *args, **kwargs)  # first argument is either `parent` or `scope`\n",
    "\n",
    "    # NOTE!! This seems brittle -- think carefully.\n",
    "    #\n",
    "    # QUESTION: Make sure to unfreeze after calling _recurse so that you don't need\n",
    "    # to set params as mutable during construction time...?\n",
    "    new_variables = scope._unfreeze_variables(variables, mutable)\n",
    "    module.scope.variables = new_variables\n",
    "\n",
    "    return module\n",
    "\n",
    "  def autonamed(self):\n",
    "    if not hasattr(self, '_autonamed'):\n",
    "      self._autonamed = {}\n",
    "    return self._autonamed\n",
    "\n",
    "  def submodules(self):\n",
    "    if not hasattr(self, '_submodules'):\n",
    "      self._submodules = {}\n",
    "    return self._submodules\n",
    "\n",
    "  def _ensure_has_name(self):\n",
    "    if not hasattr(self, 'name') or self.name is None:\n",
    "      if not hasattr(self.parent, '_in_autonames') or not self.parent._in_autonames:\n",
    "        raise ValueError(\"In order to get autonames, must decorate method with @autonames\")\n",
    "\n",
    "      self.name = \"{}/{}\".format(\n",
    "        self.__class__.__name__, \n",
    "        str(len(self.parent.autonamed())))\n",
    "      self.parent.autonamed()[self.name] = self\n",
    "\n",
    "  def __post_init__(self):\n",
    "    if isinstance(self.parent, Module):\n",
    "      self._ensure_has_name()\n",
    "      self.parent.submodules()[self.name] = self    \n",
    "      # TODO: Make scopes know of sublists, then don't call\n",
    "      # push by name here?\n",
    "      self.scope = self.parent.scope.push(self.name)\n",
    "\n",
    "    elif isinstance(self.parent, Scope):\n",
    "      self.scope = self.parent\n",
    "      \n",
    "    self.ready()\n",
    "    \n",
    "  def ready(self):\n",
    "    pass\n",
    "        \n",
    "  def copy(kwargs):\n",
    "    return self.__class__.toplevel(**dataclasses.asdict(module), **kwargs)\n",
    "\n",
    "  def variables(self):\n",
    "    return self.scope.variables\n",
    "\n",
    "  def param(self, name, init_fun, shape):\n",
    "    return self.scope.param(name, init_fun, shape)\n",
    "\n",
    "\n",
    "def autonames(fun, prefix=''):\n",
    "  @functools.wraps(fun)\n",
    "  def wrapped(self, *args, **kwargs):\n",
    "    if hasattr(self, '_autonames_fun') and self._autonames_fun != fun:\n",
    "      raise Error(\n",
    "        \"Can't only use @autonames on one method. If you want to reuse submodules across methods, \"\n",
    "        \"store submodules on `self` during `ready()`. If you want to create two sets of automatically \"\n",
    "        \"named submodules, make them submodules instead of methods.\")\n",
    "    self._autonames_fun = fun\n",
    "\n",
    "    # \"Rewind\" the autonaming process\n",
    "    self.autonamed().clear()\n",
    "\n",
    "    self._in_autonames = True\n",
    "    try:\n",
    "      return fun(self, *args, **kwargs)\n",
    "    finally:\n",
    "      self._in_autonames = False\n",
    "\n",
    "  return wrapped\n",
    "\n",
    "@dataclass\n",
    "class Dense(Module):\n",
    "  features: int\n",
    "  bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "  name: str = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    kernel = self.param('kernel', self.kernel_init, (x.shape[-1], self.features))\n",
    "    x = jnp.dot(x, kernel)\n",
    "    if self.bias:\n",
    "      x = x + self.param('bias', self.bias_init, (self.features,))\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-e5f26c4107ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Can we make this a better error message?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'features'"
     ]
    }
   ],
   "source": [
    "# TODO: Can we make this a better error message?\n",
    "Dense(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.3335357,  1.6050829, -2.0810487],\n",
       "             [-2.3335357,  1.6050829, -2.0810487],\n",
       "             [-2.3335357,  1.6050829, -2.0810487]], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: It would be nice to make this throw an error,\n",
    "# but how? I'd like to avoid requiring people to wrap /all/\n",
    "# methods in a decorator (or the similar metaclass approach with\n",
    "# hk.transparent).\n",
    "#\n",
    "# QUESTION: Can we resolve this by inspecting stack traces \n",
    "# when constucting modules, or when using them? Only during\n",
    "# \"DEBUG\" runs\n",
    "class TryReusingByNameCausesError(Module):\n",
    "  def __call__(self, x):\n",
    "    return Dense(self, 3, name=\"foo\")(x) + Dense(self, 3, name=\"foo\")(x)\n",
    "  \n",
    "try_reuse = TryReusingByNameCausesError.toplevel(rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "try_reuse(np.ones((3, 3)))\n",
    "try_reuse(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]]\n",
      "{'param': {'kernel': DeviceArray([[ 0.32717842,  0.05599118,  0.17998298],\n",
      "             [-0.12294921,  0.7071209 ,  0.28972217],\n",
      "             [ 0.1373136 , -0.02852853, -0.62830234]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}}\n",
      "[[ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "# init\n",
    "d = Dense.toplevel(3, rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "print(d(np.ones((3, 3))))\n",
    "print(d.variables())\n",
    "\n",
    "# Can call method twice on the same instance.\n",
    "print(d(np.ones((3, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.3415428 ,  0.73458356, -0.15859717],\n",
       "             [ 0.3415428 ,  0.73458356, -0.15859717],\n",
       "             [ 0.3415428 ,  0.73458356, -0.15859717]], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply\n",
    "d2 = Dense.toplevel(3, variables=d.variables())\n",
    "d2(np.ones((3, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLP(Module):\n",
    "  widths: Tuple\n",
    "  name: str = None\n",
    "\n",
    "  @autonames\n",
    "  def __call__(self, x):\n",
    "    for width in self.widths[:-1]:\n",
    "      x = nn.relu(Dense(self, width)(x))\n",
    "    x = Dense(self, self.widths[-1])(x)\n",
    "    return x\n",
    "\n",
    "  def params_by_layer_index(self, layer_index):\n",
    "    return list(self.autonamed().values())[layer_index].variables()['param']\n",
    "  \n",
    "  def params_by_layer_name(layer_name):\n",
    "    return self.submodules()[layer_name].variables()['param']\n",
    "    \n",
    "@dataclass\n",
    "class Sequential(Module):\n",
    "  layers: Tuple[Module]\n",
    "  name: str = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class MLP2(Module):\n",
    "  widths: Tuple\n",
    "  name: str = None\n",
    "\n",
    "  # QUESTION: If you implement __init__ do you need to call super.__init__ with parent and\n",
    "  # name\n",
    "  \n",
    "  @autonames\n",
    "  def ready(self):\n",
    "    # TODO: Can we make it throw an error if we do `self.layers = [Dense(self, width)] * 3`?\n",
    "    self.layers = [Dense(self, width) for width in self.widths]\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers[:-1]:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.layers[-1](x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class AutoEncoder(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "\n",
    "  # QUESTION: Should only one method be allowed to be wrapped\n",
    "  # in `autonames`?\n",
    "\n",
    "  def ready(self):\n",
    "    self.encoder = self.Encoder(self, 'encoder')\n",
    "    self.decoder = self.Decoder(self, 'decoder')\n",
    "  \n",
    "  def reconstruct(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "  \n",
    "  @dataclass\n",
    "  class Encoder(Module):\n",
    "    name: str\n",
    "    \n",
    "    @autonames\n",
    "    def __call__(self, x):\n",
    "      self.in_shape = x.shape[1:]\n",
    "      # QUESTION: Is this a legitimate use of `self.parent`?\n",
    "      for width in self.parent.encoder_widths[:-1]:\n",
    "        x = nn.relu(Dense(self, width)(x))\n",
    "      z = Dense(self, self.parent.encoder_widths[-1])(x)\n",
    "      return z\n",
    "  \n",
    "  @dataclass\n",
    "  class Decoder(Module):\n",
    "    name: str\n",
    "    \n",
    "    @autonames\n",
    "    def __call__(self, z):\n",
    "      for width in self.parent.decoder_widths[:-1]:\n",
    "        z = nn.relu(Dense(self, width)(z))\n",
    "      x = Dense(self, self.parent.encoder_widths[-1])(z)\n",
    "      # QUESITON: Is this weird? Navigating up then into encoder?\n",
    "      x = x.reshape(x.shape[:-1] + self.parent.encoder.in_shape)\n",
    "      return x\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class AutoEncoder2(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "    \n",
    "  def ready(self):\n",
    "    self.encoder = MLP2(self, self.encoder_widths, name='encode')\n",
    "    self.decoder = MLP2(self, self.decoder_widths, name='decode')\n",
    "    \n",
    "  def encode(self, x):\n",
    "    self.in_shape = x.shape[1:]\n",
    "    return self.encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    x = self.decoder(x)\n",
    "    x = x.reshape(x.shape[:-1] + self.in_shape)\n",
    "    return x\n",
    "  \n",
    "  def reconstruct(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "  \n",
    "@dataclass\n",
    "class DAE(Module):\n",
    "  encoder: Module\n",
    "  decoder: Module\n",
    "  \n",
    "  def reconstruction_loss(self, x):\n",
    "    rng = 'foo'\n",
    "    return self.loss(self.apply_noise(rng, x), self.reconstruct(x))\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "  \n",
    "  def apply_noise(self, rng, x):\n",
    "    return x\n",
    "\n",
    "  def loss(self, inputs, reconstruction):\n",
    "    return np.mean(np.abs(inputs - reconstruction))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n",
      "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
      "             [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
      "             [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}, 'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
      "             [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
      "             [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32), 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}, 'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
      "              -0.55765074],\n",
      "             [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
      "               0.41507402],\n",
      "             [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
      "              -0.84271395],\n",
      "             [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
      "              -0.15464471]], dtype=float32), 'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP.toplevel([3, 4, 5], rngs={'param': jax.random.PRNGKey(0)}, mutable=['params'])\n",
    "print(mlp(np.ones((3, 3))))\n",
    "print(mlp.variables())\n",
    " \n",
    "# QUESTION: Can you point two models to the same parameter object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n",
      "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
      "             [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
      "             [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}, 'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
      "             [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
      "             [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32), 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}, 'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
      "              -0.55765074],\n",
      "             [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
      "               0.41507402],\n",
      "             [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
      "              -0.84271395],\n",
      "             [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
      "              -0.15464471]], dtype=float32), 'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLP2.toplevel([3, 4, 5], rngs={'param': jax.random.PRNGKey(0)}, mutable=['params'])\n",
    "print(mlp2(np.ones((3, 3))))\n",
    "print(mlp2.variables())\n",
    " \n",
    "# QUESTION: Can you point two models to the same parameter object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
       "                [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
       "                [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32),\n",
       "   'bias': DeviceArray([0., 0., 0.], dtype=float32)},\n",
       "  'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
       "                [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
       "                [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32),\n",
       "   'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)},\n",
       "  'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
       "                 -0.55765074],\n",
       "                [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
       "                  0.41507402],\n",
       "                [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
       "                 -0.84271395],\n",
       "                [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
       "                 -0.15464471]], dtype=float32),\n",
       "   'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
       "              [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
       "              [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32),\n",
       " 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.params_by_layer_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLP2.toplevel([3, 4, 5], variables=mlp.variables())\n",
    "print(mlp2(np.ones((3, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634]], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a clear error if you call AutoEncoder(...) without a parent\n",
    "ae = AutoEncoder.toplevel(\n",
    "  encoder_widths=[3, 3], decoder_widths=[3, 3],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}, mutable=['params']\n",
    ")\n",
    "ae.reconstruct(np.ones((4, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.0273236 , -0.30911958, -0.9112693 ],\n",
       "             [ 0.0273236 , -0.30911958, -0.9112693 ],\n",
       "             [ 0.0273236 , -0.30911958, -0.9112693 ],\n",
       "             [ 0.0273236 , -0.30911958, -0.9112693 ]], dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a clear error if you call AutoEncoder(...) without a parent\n",
    "ae2 = AutoEncoder2.toplevel(\n",
    "  encoder_widths=[3, 3], decoder_widths=[3, 3],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}, mutable=['params']\n",
    ")\n",
    "ae2.reconstruct(np.ones((4, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634]], dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# QUESTION: Does this work?!\n",
    "# should this error? We're connecting submodules of another module into here.\n",
    "# we should either think carefully about what kind of (both good and bad) behavior this\n",
    "# may lead to. Or if we're not sure we can make it raise an Error.\n",
    "dae = DAE.toplevel(\n",
    "  encoder=ae.encoder, decoder=ae.decoder,\n",
    "  rngs={'param': jax.random.PRNGKey(0)}, mutable=['param']\n",
    ")\n",
    "dae.reconstruct(np.ones((4, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "X = np.ones((1, 10))\n",
    "Y = np.ones((5, ))\n",
    "\n",
    "@jit\n",
    "def predict(params):\n",
    "  # TODO: Think about the fact that you have to put the hyperparameters here  \n",
    "  mlp = MLP.toplevel([3, 4, 5], variables={'param': params})\n",
    "  return mlp(X)\n",
    "  \n",
    "@jit\n",
    "def loss_fn(params):\n",
    "  Yhat = predict(params)\n",
    "  # TODO: Print in jit\n",
    "  return jnp.mean(jnp.abs(Y - Yhat))\n",
    "\n",
    "@jit\n",
    "def init_params(rng):\n",
    "  # TODO: Think about the fact that you have to put the hyperparameters here  \n",
    "  mlp = MLP.toplevel([3, 4, 5], rngs={'param': rng}, mutable=['param'])\n",
    "  mlp(X)\n",
    "  return mlp.variables()['param']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1.4570823, dtype=float32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(init_params(jax.random.PRNGKey(42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense/0': {'bias': DeviceArray([0.25291714, 0.        , 0.        ], dtype=float32),\n",
       "  'kernel': DeviceArray([[0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ]], dtype=float32)},\n",
       " 'Dense/1': {'bias': DeviceArray([ 0.04557207, -0.20579326,  0.5184991 ,  0.        ], dtype=float32),\n",
       "  'kernel': DeviceArray([[ 0.08235972, -0.37191805,  0.9370529 ,  0.        ],\n",
       "               [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "               [ 0.        , -0.        ,  0.        ,  0.        ]],            dtype=float32)},\n",
       " 'Dense/2': {'bias': DeviceArray([-0.2, -0.2, -0.2, -0.2, -0.2], dtype=float32),\n",
       "  'kernel': DeviceArray([[-0.2021582 , -0.2021582 , -0.2021582 , -0.2021582 ,\n",
       "                -0.2021582 ],\n",
       "               [-0.19387028, -0.19387028, -0.19387028, -0.19387028,\n",
       "                -0.19387028],\n",
       "               [-0.23548912, -0.23548912, -0.23548912, -0.23548912,\n",
       "                -0.23548912],\n",
       "               [-0.        , -0.        , -0.        , -0.        ,\n",
       "                -0.        ]], dtype=float32)}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(loss_fn)(init_params(jax.random.PRNGKey(42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss =  1.4570823 Yhat =  [[-0.65048295 -0.89209783  0.22747914 -0.8393059  -0.13100383]]\n",
      "1 loss =  1.3724773 Yhat =  [[-0.58094865 -0.7692777   0.25747335 -0.68587863 -0.08375458]]\n",
      "2 loss =  1.2976424 Yhat =  [[-0.5199375  -0.6605372   0.28393385 -0.5494852  -0.04218574]]\n",
      "3 loss =  1.2308433 Yhat =  [[-0.46609223 -0.5635818   0.30782127 -0.42721057 -0.00515322]]\n",
      "4 loss =  1.1706475 Yhat =  [[-0.41830716 -0.47650898  0.32992083 -0.31662038  0.02827771]]\n",
      "5 loss =  1.1158575 Yhat =  [[-0.37567115 -0.39772335  0.3508847  -0.21565796  0.058881  ]]\n",
      "6 loss =  1.0654583 Yhat =  [[-0.3374258  -0.32587245  0.37126485 -0.1225654   0.08730697]]\n",
      "7 loss =  1.01858 Yhat =  [[-0.3029324  -0.2597958   0.39153802 -0.03582046  0.11411095]]\n",
      "8 loss =  0.9744629 Yhat =  [[-0.27164698 -0.19848418  0.41212633  0.04591412  0.13977619]]\n",
      "9 loss =  0.93243355 Yhat =  [[-0.24309914 -0.14104642  0.4334131   0.12383318  0.1647319 ]]\n",
      "10 loss =  0.89188176 Yhat =  [[-0.21687557 -0.08668173  0.4557565   0.19902366  0.18936858]]\n",
      "11 loss =  0.8522431 Yhat =  [[-0.19260697 -0.03465696  0.47950214  0.2724954   0.21405075]]\n",
      "12 loss =  0.8129824 Yhat =  [[-0.16995552  0.01571459  0.5049928   0.34520784  0.2391284 ]]\n",
      "13 loss =  0.7735786 Yhat =  [[-0.1486052   0.0650912   0.53257906  0.4180948   0.26494727]]\n",
      "14 loss =  0.7335111 Yhat =  [[-0.12825191  0.11412009  0.5626294   0.49208808  0.29185894]]\n",
      "15 loss =  0.69224566 Yhat =  [[-0.10859458  0.16345426  0.59554034  0.56814075  0.3202309 ]]\n",
      "16 loss =  0.64922005 Yhat =  [[-0.08932619  0.21376987  0.63174784  0.64725125  0.35045698]]\n",
      "17 loss =  0.60382843 Yhat =  [[-0.07012403  0.26578405  0.6717398   0.73048896  0.38296917]]\n",
      "18 loss =  0.5554043 Yhat =  [[-0.05063945  0.32027492  0.7160703   0.81902236  0.41825047]]\n",
      "19 loss =  0.5032006 Yhat =  [[-0.0304857   0.37810406  0.7653772   0.9141513   0.45685035]]\n",
      "20 loss =  0.4533039 Yhat =  [[-0.00922396  0.440243    0.82040274  1.0173447   0.4994034 ]]\n",
      "21 loss =  0.4309895 Yhat =  [[0.01550364 0.46210483 0.847936   1.0037661  0.5232741 ]]\n",
      "22 loss =  0.41142565 Yhat =  [[0.04048632 0.48488075 0.87775767 0.9911665  0.54858065]]\n",
      "23 loss =  0.3881277 Yhat =  [[0.06578112 0.55670345 0.9447177  1.107753   0.5999121 ]]\n",
      "24 loss =  0.3618575 Yhat =  [[0.09341697 0.5834149  0.98066366 1.0965475  0.62976426]]\n",
      "25 loss =  0.34212318 Yhat =  [[0.12192378 0.61176026 1.0197643  1.08631    0.66177446]]\n",
      "26 loss =  0.32251656 Yhat =  [[0.15782161 0.62394416 0.97272    1.0388074  0.67173886]]\n",
      "27 loss =  0.2996928 Yhat =  [[0.18685192 0.65398157 1.0150666  1.0305666  0.7063358 ]]\n",
      "28 loss =  0.28714204 Yhat =  [[0.22153574 0.6666831  0.9716505  0.98671955 0.71770096]]\n",
      "29 loss =  0.2751517 Yhat =  [[0.2512446  0.7449127  1.0530101  1.1016239  0.78271824]]\n",
      "30 loss =  0.24511957 Yhat =  [[0.28479502 0.7549197  1.0101901  1.0509984  0.795876  ]]\n",
      "31 loss =  0.22798 Yhat =  [[0.31760946 0.76586896 0.9708903  1.0043008  0.8100322 ]]\n",
      "32 loss =  0.20246647 Yhat =  [[0.35366696 0.80707824 1.0232872  1.0050415  0.85525113]]\n",
      "33 loss =  0.19412702 Yhat =  [[0.38784522 0.82078844 0.986677   0.9615653  0.8724889 ]]\n",
      "34 loss =  0.1745839 Yhat =  [[0.4208019  0.90355206 1.0889902  1.0677395  0.95945626]]\n",
      "35 loss =  0.14233367 Yhat =  [[0.46075532 0.9234277  1.0510045  1.0261015  0.98125464]]\n",
      "36 loss =  0.11812698 Yhat =  [[0.5004256 0.9439093 1.0159982 0.9858423 1.0048138]]\n",
      "37 loss =  0.10780936 Yhat =  [[0.53051573 0.97743434 0.9926726  1.0205092  0.9808398 ]]\n",
      "38 loss =  0.11313243 Yhat =  [[0.5775589 1.0300927 1.0559261 1.0173239 1.0398784]]\n",
      "39 loss =  0.1388135 Yhat =  [[0.6040769  0.9153841  0.96057457 0.8766802  0.9492167 ]]\n",
      "40 loss =  0.09366264 Yhat =  [[0.6617912 1.025131  1.0551342 1.0108018 1.0390375]]\n",
      "41 loss =  0.11912451 Yhat =  [[0.69142056 0.92219335 0.9548528  0.89015186 0.9457589 ]]\n",
      "42 loss =  0.076585196 Yhat =  [[0.74912   1.0219318 1.0613147 1.0048676 1.043932 ]]\n",
      "43 loss =  0.10120372 Yhat =  [[0.7769967  0.91941136 0.96161956 0.8848545  0.95109934]]\n",
      "44 loss =  0.060259115 Yhat =  [[0.84106094 1.0205176  1.0699041  1.0009768  1.050958  ]]\n",
      "45 loss =  0.080880515 Yhat =  [[0.86758685 0.9180553  0.97049177 0.88118565 0.9582779 ]]\n",
      "46 loss =  0.045232058 Yhat =  [[0.93735576 1.0188435  1.0808176  0.9959657  1.0598207 ]]\n",
      "47 loss =  0.019267917 Yhat =  [[0.9699148 0.9587252 1.0203396 0.997553  1.002193 ]]\n",
      "48 loss =  0.016235663 Yhat =  [[1.0126035 1.0034362 0.9998765 1.045267  0.9802519]]\n",
      "49 loss =  0.02005595 Yhat =  [[0.96742135 0.9561681  1.0180684  0.99442583 1.0002266 ]]\n"
     ]
    }
   ],
   "source": [
    "params = init_params(jax.random.PRNGKey(42))\n",
    "for i in range(50):\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  print(i, \"loss = \", loss, \"Yhat = \", predict(params))\n",
    "  lr = 0.03\n",
    "  params = jax.tree_multimap(lambda x, d: x - lr * d, params, grad)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DenseExplicit(Module):\n",
    "  in_features: int\n",
    "  out_features: int\n",
    "  with_bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "  name: str = None\n",
    "\n",
    "  def ready(self):\n",
    "    self.kernel = self.param('kernel', self.kernel_init, (self.in_features, self.out_features))\n",
    "\n",
    "    if self.with_bias:\n",
    "      self.bias = self.param('bias', self.bias_init, (self.out_features,))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    x = jnp.dot(x, self.kernel)\n",
    "    if self.with_bias:\n",
    "      x = x + self.bias\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6191777  0.8351118 -0.5551028]\n"
     ]
    }
   ],
   "source": [
    "dense_expl = DenseExplicit.toplevel(\n",
    "  in_features=3, out_features=3,\n",
    "  # TODO: Does this work correctly without mutable=['param']?\n",
    "  rngs={'param': jax.random.PRNGKey(1)}\n",
    ")\n",
    "print(dense_expl(np.ones((3, ))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLPExplicit(Module):\n",
    "  features: List[int]\n",
    "\n",
    "  def ready(self):\n",
    "    self.layers = [\n",
    "      DenseExplicit(self, self.features[i], self.features[i+1], name='dense' + str(i))\n",
    "      for i in range(len(self.features)-1)\n",
    "    ]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for l in self.layers[:-1]:\n",
    "      x = nn.relu(l(x))\n",
    "    return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE! We don't need to set mutable=['param'] here\n",
    "mlp_expl = MLPExplicit.toplevel(\n",
    "  features=[3, 4, 5, 6],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.33702424,  1.1153386 ,  0.30553615,  1.1103984 ,\n",
       "              -0.02722588],\n",
       "             [-0.17242931, -0.11199684, -0.19800997,  0.32914558,\n",
       "              -0.6227858 ],\n",
       "             [-0.9789032 , -0.11746633,  0.940915  ,  0.8141851 ,\n",
       "               0.662503  ],\n",
       "             [-0.24981995,  1.0711411 , -0.6310842 , -0.10951854,\n",
       "               0.3269265 ]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_expl.layers[1].kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLPExplicit2(Module):\n",
    "  features: List[int]\n",
    "\n",
    "  @autonames\n",
    "  def ready(self):\n",
    "    self.layers = [\n",
    "      DenseExplicit(self, self.features[i], self.features[i+1])\n",
    "      for i in range(len(self.features)-1)\n",
    "    ]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for l in self.layers[:-1]:\n",
    "      x = nn.relu(l(x))\n",
    "    return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.33702424,  1.1153386 ,  0.30553615,  1.1103984 ,\n",
       "              -0.02722588],\n",
       "             [-0.17242931, -0.11199684, -0.19800997,  0.32914558,\n",
       "              -0.6227858 ],\n",
       "             [-0.9789032 , -0.11746633,  0.940915  ,  0.8141851 ,\n",
       "               0.662503  ],\n",
       "             [-0.24981995,  1.0711411 , -0.6310842 , -0.10951854,\n",
       "               0.3269265 ]], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPExplicit.toplevel(\n",
    "  features=[3, 4, 5, 6],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}\n",
    ").layers[1].kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2.4221723 , -2.1911113 , -0.10372341, -3.2877061 ,\n",
       "              -0.14353384, -1.4626168 ],\n",
       "             [ 2.4221723 , -2.1911113 , -0.10372341, -3.2877061 ,\n",
       "              -0.14353384, -1.4626168 ],\n",
       "             [ 2.4221723 , -2.1911113 , -0.10372341, -3.2877061 ,\n",
       "              -0.14353384, -1.4626168 ]], dtype=float32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_expl(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_weight(module):\n",
    "  @dataclass\n",
    "  class StdWeight(Module):\n",
    "    initialized: bool = False\n",
    "    \n",
    "    def __call__(self, x):\n",
    "      if not self.params():\n",
    "        # initialize parameters\n",
    "        module(x)\n",
    "      \n",
    "      param = module.variables.param\n",
    "      # TODO: Test that I would get an error if I directly modified `param`\n",
    "      param = param.copy(kernel=std(param['kernel']))\n",
    "\n",
    "      def with_vars(variables):\n",
    "        # QUESTION: Can `with_vars` be implemented without assuming\n",
    "        # that modules are dataclasses?\n",
    "        module.__class__.toplevel(\n",
    "          **dataclasses.asdict(module), variables=variables)\n",
    "      return with_vars({'param': param})(x)\n",
    "  return StdWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StdWeight:\n",
    "  module: Module\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    if not self.module.params():\n",
    "      # initialize parameters\n",
    "      self.module(x)\n",
    "\n",
    "    param = self.module.variables.param\n",
    "    # TODO: Test that I would get an error if I directly modified `param`\n",
    "    std_param = param.copy(kernel=std(param['kernel']))\n",
    "    return module.copy({'param': std_param})(x)\n",
    "\n",
    "///\n",
    "class MyModule:\n",
    "  def foo(self, x):\n",
    "    module = Dense(self, 3)\n",
    "    std_module = StdWeight(module)\n",
    "    std_module(x)  # parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

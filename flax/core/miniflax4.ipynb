{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "from typing import Any, Callable, Iterable, Optional, Tuple, Type\n",
    "from flax import nn\n",
    "from flax.nn import initializers\n",
    "from flax.core.scope import Scope\n",
    "from flax.core import scope\n",
    "from jax import numpy as jnp\n",
    "import functools\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "\n",
    "from flax.core.frozen_dict import freeze\n",
    "\n",
    "@dataclass\n",
    "class Module:\n",
    "  parent: Optional[Type[\"Module\"]]\n",
    "  \n",
    "  @classmethod\n",
    "  def toplevel(cls, *args, rngs=None, variables=None, mutable=False, **kwargs):\n",
    "    # TODO: Think about the fact that `rngs` and `params` live on args\n",
    "    # and kwargs\n",
    "    if rngs is None:\n",
    "      rngs = {}\n",
    "    if variables is None:\n",
    "      variables = {'param': {}}\n",
    "    module = cls(None, *args, **kwargs)  # first argument is `parent` dataclass attr\n",
    "    new_variables = scope._unfreeze_variables(variables, mutable)\n",
    "    module._scope = Scope(new_variables, rngs=rngs)\n",
    "    return module\n",
    "\n",
    "  def autonamed(self, prefix=''):\n",
    "    if not hasattr(self, '_autonamed'):\n",
    "      self._autonamed = {}\n",
    "    if not prefix in self._autonamed:\n",
    "      self._autonamed[prefix] = {}\n",
    "    return self._autonamed[prefix]\n",
    "\n",
    "  def submodules(self):\n",
    "    if not hasattr(self, '_submodules'):\n",
    "      self._submodules = {}\n",
    "    return self._submodules\n",
    "\n",
    "  def _autoname(self):\n",
    "    self.name = self.parent._dynamic_autoname_prefix + \"{}/{}\".format(\n",
    "      self.__class__.__name__, \n",
    "      str(len(self.parent.autonamed())))\n",
    "    self.parent.autonamed(self.parent._dynamic_autoname_prefix)[self.name] = self\n",
    "  \n",
    "  def _init_scope(self):\n",
    "    if self.parent is None:\n",
    "      raise ValueError(\n",
    "        'Trying to create a module instance at the top-level? '\n",
    "        'Use, e.g. `MyModule.toplevel(...)`')\n",
    "\n",
    "    if not hasattr(self, 'name') or self.name is None:\n",
    "      if hasattr(self.parent, '_dynamic_autoname_prefix') and self.parent._dynamic_autoname_prefix is not None:\n",
    "        self._autoname()\n",
    "      else:\n",
    "        raise ValueError(\"To use automatically named submodules, wrap your method in `@autonames`.\")\n",
    "\n",
    "    self.parent.submodules()[self.name] = self    \n",
    "\n",
    "    # TODO: Make scopes know of sublists, then don't call\n",
    "    # push by name here.\n",
    "    self._scope = self.parent._scope.push(self.name)\n",
    "  \n",
    "  def scope(self):\n",
    "    if not hasattr(self, '_scope'):\n",
    "      self._init_scope()\n",
    "    return self._scope\n",
    "    \n",
    "  def variables(self):\n",
    "    return self.scope().variables\n",
    "\n",
    "  def param(self, name, init_fun, shape):\n",
    "    return self.scope().param(name, init_fun, shape)\n",
    "\n",
    "\n",
    "def _autonames(prefix=''):\n",
    "  def _wrap(fun):\n",
    "    @functools.wraps(fun)\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "      if not hasattr(self, '_autonames_fun'):\n",
    "        self._autonames_fun= {}\n",
    "      if prefix in self._autonames_fun and self._autonames_fun[prefix] != fun:\n",
    "        raise Error(\n",
    "          \"To use @autonames on more than one method, \"\n",
    "          \"you must give each (other than one) a unique prefix \"\n",
    "          \"via the `prefix` argument to @autonames.\")\n",
    "      self._autonames_fun[prefix] = fun\n",
    "\n",
    "      # \"Rewind\" the autonaming process\n",
    "      # NOTE: This might be worth documenting; if you store attributes on submodules during a call to, say, __call__()\n",
    "      # and then modify them from the outside (if __call__ returned the module instances), then the next time\n",
    "      # that you run that __call__, you will no longer have those attributes, because you're creating new instances of\n",
    "      # submodules.\n",
    "      \n",
    "      # QUESTION: Should we bring the cursor back to 0 instead of clearing? Then we can re-use the same module instances\n",
    "      # and it would presumably be faster (during jit time?)\n",
    "      self.autonamed(prefix).clear()\n",
    "\n",
    "      self._dynamic_autoname_prefix = prefix\n",
    "      try:\n",
    "        return fun(self, *args, **kwargs)\n",
    "      finally:\n",
    "        self._dynamic_autoname_prefix = None\n",
    "\n",
    "    return wrapped\n",
    "  return _wrap\n",
    "\n",
    "autonames = _autonames()\n",
    "autonames.prefix = _autonames\n",
    "\n",
    "@dataclass\n",
    "class Dense(Module):\n",
    "  features: int\n",
    "  bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "  name: str = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    kernel = self.param('kernel', self.kernel_init, (x.shape[-1], self.features))\n",
    "    x = jnp.dot(x, kernel)\n",
    "    if self.bias:\n",
    "      x = x + self.param('bias', self.bias_init, (self.features,))\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/jax/lib/xla_bridge.py:122: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.1667678 ,  0.80254143, -1.0405244 ],\n",
       "             [-1.1667678 ,  0.80254143, -1.0405244 ],\n",
       "             [-1.1667678 ,  0.80254143, -1.0405244 ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure you can call a method twice that has named submodules\n",
    "@dataclass\n",
    "class TestCallTwiceWithNamedSubmodule(Module):\n",
    "  name: Optional[str] = None\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    return Dense(self, 3, name=\"foo\")(x)\n",
    "  \n",
    "try_reuse = TestCallTwiceWithNamedSubmodule.toplevel(3, rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "try_reuse(np.ones((3, 3)))\n",
    "try_reuse(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this a better error message?\n",
    "# Dense(features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.3335357,  1.6050829, -2.0810487],\n",
       "             [-2.3335357,  1.6050829, -2.0810487],\n",
       "             [-2.3335357,  1.6050829, -2.0810487]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: It would be nice to make this throw an error,\n",
    "# but how? I'd like to avoid requiring people to wrap /all/\n",
    "# methods in a decorator (or the similar metaclass approach with\n",
    "# hk.transparent).\n",
    "#\n",
    "# QUESTION: Can we resolve this by inspecting stack traces \n",
    "# when constucting modules, or when using them? Only during\n",
    "# \"DEBUG\" runs\n",
    "@dataclass\n",
    "class TryReusingByNameCausesError(Module):\n",
    "  name: Optional[str]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return Dense(self, 3, name=\"foo\")(x) + Dense(self, 3, name=\"foo\")(x)\n",
    "  \n",
    "try_reuse = TryReusingByNameCausesError.toplevel(3, rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "try_reuse(np.ones((3, 3)))\n",
    "try_reuse(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]]\n",
      "{'param': {'kernel': DeviceArray([[ 0.32717842,  0.05599118,  0.17998298],\n",
      "             [-0.12294921,  0.7071209 ,  0.28972217],\n",
      "             [ 0.1373136 , -0.02852853, -0.62830234]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}}\n",
      "[[ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]]\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "d = Dense.toplevel(3, rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "print(d(np.ones((3, 3))))\n",
    "print(d.variables())\n",
    "\n",
    "# Can call method twice on the same instance.\n",
    "print(d(np.ones((3, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.3415428 ,  0.73458356, -0.15859717],\n",
       "             [ 0.3415428 ,  0.73458356, -0.15859717],\n",
       "             [ 0.3415428 ,  0.73458356, -0.15859717]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply\n",
    "d2 = Dense.toplevel(3, variables=d.variables())\n",
    "d2(np.ones((3, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLP(Module):\n",
    "  widths: Tuple\n",
    "  name: str = None\n",
    "\n",
    "  @autonames\n",
    "  def __call__(self, x):\n",
    "    for width in self.widths[:-1]:\n",
    "      x = nn.relu(Dense(self, width)(x))\n",
    "    x = Dense(self, self.widths[-1])(x)\n",
    "    return x\n",
    "\n",
    "  def params_by_layer_index(self, layer_index):\n",
    "    return list(self.autonamed().values())[layer_index].variables()['param']\n",
    "  \n",
    "  def params_by_layer_name(layer_name):\n",
    "    return self.submodules()[layer_name].variables()['param']\n",
    "    \n",
    "@dataclass\n",
    "class Sequential(Module):\n",
    "  layers: Tuple[Module]\n",
    "  name: str = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class MLP2(Module):\n",
    "  widths: Tuple\n",
    "  name: str = None\n",
    "\n",
    "  # QUESTION: If you implement __init__ do you need to call super.__init__ with parent and\n",
    "  # name_or_list\n",
    "  \n",
    "  @autonames\n",
    "  def __post_init__(self):\n",
    "    self.layers = [Dense(self, width) for width in self.widths]\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers[:-1]:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.layers[-1](x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class AutoEncoder(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "  \n",
    "  @autonames.prefix('encoder:')\n",
    "  def encode(self, x):\n",
    "    self.in_shape = x.shape[1:]\n",
    "    for width in self.encoder_widths[:-1]:\n",
    "      x = nn.relu(Dense(self, width)(x))\n",
    "    z = Dense(self, self.encoder_widths[-1])(x)\n",
    "    return z\n",
    "\n",
    "  @autonames.prefix('decoder:')\n",
    "  def decode(self, z):\n",
    "    for width in self.decoder_widths[:-1]:\n",
    "      z = nn.relu(Dense(self, width)(z))\n",
    "    x = Dense(self, self.decoder_widths[-1])(z)\n",
    "    x = x.reshape(x.shape[:-1] + self.in_shape)\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class AutoEncoder2(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "    \n",
    "  @autonames\n",
    "  def __post_init__(self):\n",
    "    self.encoder = MLP2(self, 'encode', self.encoder_widths)\n",
    "    self.decoder = MLP2(self, 'decode', self.decoder_widths)\n",
    "    \n",
    "  def encode(self, x):\n",
    "    return self.encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    x = self.decoder(x)\n",
    "    x = x.reshape(x.shape[:-1] + self.in_shape)\n",
    "    return x\n",
    "  \n",
    "@dataclass\n",
    "class DenoisingAutoEncoder3(Module):\n",
    "  encoder: Module\n",
    "  decoder: Module\n",
    "  \n",
    "  def reconstruction_loss(self, x):\n",
    "    rng = 'foo'\n",
    "    return self.loss(self.apply_noise(rng, x), self.reconstruct(x))\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "  \n",
    "  def apply_noise(self, rng, x):\n",
    "    return x\n",
    "\n",
    "  def loss(self, inputs, reconstruction):\n",
    "    return np.mean(np.abs(inputs - reconstruction))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n",
      "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
      "             [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
      "             [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}, 'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
      "             [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
      "             [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32), 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}, 'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
      "              -0.55765074],\n",
      "             [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
      "               0.41507402],\n",
      "             [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
      "              -0.84271395],\n",
      "             [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
      "              -0.15464471]], dtype=float32), 'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP.toplevel([3, 4, 5], rngs={'param': jax.random.PRNGKey(0)}, mutable=['params'])\n",
    "print(mlp(np.ones((3, 3))))\n",
    "print(mlp.variables())\n",
    " \n",
    "# QUESTION: Can you point two models to the same parameter object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
       "                [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
       "                [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32),\n",
       "   'bias': DeviceArray([0., 0., 0.], dtype=float32)},\n",
       "  'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
       "                [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
       "                [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32),\n",
       "   'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)},\n",
       "  'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
       "                 -0.55765074],\n",
       "                [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
       "                  0.41507402],\n",
       "                [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
       "                 -0.84271395],\n",
       "                [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
       "                 -0.15464471]], dtype=float32),\n",
       "   'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
       "              [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
       "              [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32),\n",
       " 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.params_by_layer_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "To use automatically named submodules, wrap your method in `@autonames`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c3033b1cabe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoplevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-541d6be7f2e3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f2a9cdf9ba34>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f2a9cdf9ba34>\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fun, shape)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f2a9cdf9ba34>\u001b[0m in \u001b[0;36mscope\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_scope'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f2a9cdf9ba34>\u001b[0m in \u001b[0;36m_init_scope\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autoname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To use automatically named submodules, wrap your method in `@autonames`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: To use automatically named submodules, wrap your method in `@autonames`."
     ]
    }
   ],
   "source": [
    "mlp2 = MLP2.toplevel([3, 4, 5], variables=mlp.variables())\n",
    "print(mlp2(np.ones((3, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a clear error if you call AutoEncoder(...) without a parent\n",
    "ae = AutoEncoder.toplevel(\n",
    "  encoder_widths=[3, 3], decoder_widths=[3, 3],\n",
    "  rngs={'param': jax.random.PRNGKey(0)}, mutable=['params']\n",
    ")\n",
    "ae.reconstruct(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# should this error? We're connecting submodules of another module into here.\n",
    "# we should either think carefully about what kind of (both good and bad) behavior this\n",
    "# may lead to. Or if we're not sure we can make it raise an Error.\n",
    "dae = DAE.toplevel(\n",
    "  encoder=ae2.encoder, decoder=ae2.decoder,\n",
    "  prngs={'param': jax.random.PRNGKey(0)}, mutable=['param']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "X = np.ones((1, 10))\n",
    "Y = np.ones((5, ))\n",
    "\n",
    "@jit\n",
    "def predict(params):\n",
    "  # TODO: Think about the fact that you have to put the hyperparameters here  \n",
    "  mlp = MLP.toplevel([3, 4, 5], variables={'param': params})\n",
    "  return mlp(X)\n",
    "  \n",
    "@jit\n",
    "def loss_fn(params):\n",
    "  Yhat = predict(params)\n",
    "  # TODO: Print in jit\n",
    "  return jnp.mean(jnp.abs(Y - Yhat))\n",
    "\n",
    "@jit\n",
    "def init_params(rng):\n",
    "  # TODO: Think about the fact that you have to put the hyperparameters here  \n",
    "  mlp = MLP.toplevel([3, 4, 5], rngs={'param': rng}, mutable=['param'])\n",
    "  mlp(X)\n",
    "  return mlp.variables()['param']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(init_params(jax.random.PRNGKey(42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.grad(loss_fn)(init_params(jax.random.PRNGKey(42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_params(jax.random.PRNGKey(42))\n",
    "for i in range(50):\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  print(i, \"loss = \", loss, \"Yhat = \", predict(params))\n",
    "  lr = 0.03\n",
    "  params = jax.tree_multimap(lambda x, d: x - lr * d, params, grad)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

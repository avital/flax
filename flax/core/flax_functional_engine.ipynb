{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0SPwYS9dtYA"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import jax\n",
    "from jax import numpy as jnp, random, lax\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n9cxyCzluvI"
   },
   "outputs": [],
   "source": [
    "from flax import nn, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0L7YCrobkfzU"
   },
   "outputs": [],
   "source": [
    "from flax.core.scope import Scope, init, apply, Array, group_kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1116,
     "status": "ok",
     "timestamp": 1590673431275,
     "user": {
      "displayName": "Jonathan Heek",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
      "userId": "00491914421152177709"
     },
     "user_tz": -120
    },
    "id": "aDLGb3iGkjoL",
    "outputId": "2558605e-e485-407e-b062-74d31cc49f1e"
   },
   "outputs": [],
   "source": [
    "def dense(scope: Scope, inputs: Array, features: int, bias: bool = True,\n",
    "          kernel_init=nn.linear.default_kernel_init,\n",
    "          bias_init=nn.initializers.zeros):\n",
    "  kernel = scope.param('kernel', kernel_init, (inputs.shape[-1], features))\n",
    "  y = jnp.dot(inputs, kernel)\n",
    "  if bias:\n",
    "    y += scope.param('bias', bias_init, (features,))\n",
    "  return y\n",
    "\n",
    "model_fn = functools.partial(dense, features=3)\n",
    "\n",
    "x = jnp.ones((1, 2))\n",
    "y, params = init(model_fn)(random.PRNGKey(0), x)\n",
    "print(params)\n",
    "\n",
    "def mlp(scope: Scope, inputs: Array, features: int):\n",
    "  hidden = dense(scope.push('hidden'), inputs, features)\n",
    "  hidden = nn.relu(hidden)\n",
    "  return dense(scope.push('out'), hidden, 1)\n",
    "\n",
    "init(mlp)(random.PRNGKey(0), x, features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1590672862325,
     "user": {
      "displayName": "Jonathan Heek",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
      "userId": "00491914421152177709"
     },
     "user_tz": -120
    },
    "id": "SHoB5lomZp9s",
    "outputId": "d2898913-ece5-434e-eedf-249324df1775"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, Any\n",
    "\n",
    "def dense_general(\n",
    "    scope: Scope, inputs: Array, features: int, axis = -1,\n",
    "    batch_dims=(), bias=True, dtype=jnp.float32,\n",
    "    kernel_init=nn.linear.default_kernel_init, bias_init=nn.initializers.zeros,\n",
    "    precision=None):\n",
    "  \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n",
    "\n",
    "  Args:\n",
    "    scope: module scope\n",
    "    inputs: The nd-array to be transformed.\n",
    "    features: tuple with numbers of output features.\n",
    "    axis: tuple with axes to apply the transformation on.\n",
    "    batch_dims: tuple with batch axes.\n",
    "    bias: whether to add a bias to the output (default: True).\n",
    "    dtype: the dtype of the computation (default: float32).\n",
    "    kernel_init: initializer function for the weight matrix.\n",
    "    bias_init: initializer function for the bias.\n",
    "    precision: numerical precision of the computation see `jax.lax.Precision`\n",
    "      for details.\n",
    "  Returns:\n",
    "    The transformed input.\n",
    "  \"\"\"\n",
    "  inputs = jnp.asarray(inputs, dtype)\n",
    "  features = _as_tuple(features)\n",
    "  axis = _as_tuple(axis)\n",
    "  batch_dims = _as_tuple(batch_dims)\n",
    "  \n",
    "  if batch_dims:\n",
    "    max_dim = onp.max(batch_dims)\n",
    "    if set(batch_dims) != set(range(max_dim + 1)):\n",
    "      raise ValueError(f'batch_dims {batch_dims} must be consecutive leading '\n",
    "                       'dimensions starting from 0.')\n",
    "      \n",
    "  ndim = inputs.ndim\n",
    "  n_batch_dims = len(batch_dims)\n",
    "  axis = _normalize_axes(axis, ndim)\n",
    "  batch_dims = _normalize_axes(batch_dims, ndim)\n",
    "  n_axis, n_features = len(axis), len(features)\n",
    "\n",
    "  batch_shape = tuple([inputs.shape[ax] for ax in batch_dims])\n",
    "  kernel_shape = tuple([inputs.shape[ax] for ax in axis]) + features\n",
    "  kernel = scope.param('kernel', kernel_init, batch_shape + kernel_shape)\n",
    "  kernel = jnp.asarray(kernel, dtype)\n",
    "\n",
    "  batch_ind = tuple(range(n_batch_dims))\n",
    "  contract_ind = tuple(range(n_batch_dims, n_axis + n_batch_dims))\n",
    "  out = lax.dot_general(inputs,\n",
    "                        kernel,\n",
    "                        ((axis, contract_ind), (batch_dims, batch_ind)),\n",
    "                        precision=precision)\n",
    "  if bias:\n",
    "    bias = scope.param('bias', bias_init, batch_shape + features)\n",
    "\n",
    "    # Reshape bias for broadcast.\n",
    "    expand_dims = sorted(\n",
    "        set(range(inputs.ndim)) - set(axis) - set(batch_dims))\n",
    "    for ax in expand_dims:\n",
    "      bias = jnp.expand_dims(bias, ax)\n",
    "    bias = jnp.asarray(bias, dtype)\n",
    "    out = out + bias\n",
    "  return out\n",
    "\n",
    "def _as_tuple(x):\n",
    "  if isinstance(x, Iterable):\n",
    "    return tuple(x)\n",
    "  return (x,)\n",
    "\n",
    "def _normalize_axes(axes, ndim):\n",
    "  # A tuple by convention. len(axes_tuple) then also gives the rank efficiently.\n",
    "  return tuple([ax if ax >= 0 else ndim + ax for ax in axes])\n",
    "\n",
    "init(dense_general)(random.PRNGKey(0), jnp.ones((2, 3,)), features=1, batch_dims=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHKL_QOBa-IK"
   },
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class CacheEntry:\n",
    "  key: onp.ndarray\n",
    "  value: onp.ndarray\n",
    "  i: onp.ndarray\n",
    "\n",
    "def multi_head_dot_product_attention(\n",
    "    scope: Scope,\n",
    "          inputs_q,\n",
    "          inputs_kv,\n",
    "          num_heads,\n",
    "          dtype=jnp.float32,\n",
    "          qkv_features=None,\n",
    "          out_features=None,\n",
    "          attention_axis=None,\n",
    "          causal_mask=False,\n",
    "          padding_mask=None,\n",
    "          key_padding_mask=None,\n",
    "          segmentation=None,\n",
    "          key_segmentation=None,\n",
    "          cache=False,\n",
    "          broadcast_dropout=True,\n",
    "          dropout_rng=None,\n",
    "          dropout_rate=0.,\n",
    "          deterministic=False,\n",
    "          precision=None,\n",
    "          kernel_init=nn.attention.default_kernel_init,\n",
    "          bias_init=nn.initializers.zeros,\n",
    "          bias=True,\n",
    "          attention_fn=nn.attention.dot_product_attention):\n",
    "  \"\"\"Applies multi-head dot product attention on the input data.\n",
    "\n",
    "  Projects the inputs into multi-headed query, key, and value vectors,\n",
    "  applies dot-product attention and project the results to an output vector.\n",
    "\n",
    "  This can be used for encoder-decoder attention by specifying both `inputs_q`\n",
    "  and `inputs_kv` orfor self-attention by only specifying `inputs_q` and\n",
    "  setting `inputs_kv` to None.\n",
    "\n",
    "  Args:\n",
    "    inputs_q: input queries of shape `[bs, dim1, dim2, ..., dimN, features]`.\n",
    "    inputs_kv: key/values of shape `[bs, dim1, dim2, ..., dimN, features]`\n",
    "      or None for self-attention, inn which case key/values will be derived\n",
    "      from inputs_q.\n",
    "    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n",
    "      should be divisible by the number of heads.\n",
    "    dtype: the dtype of the computation (default: float32)\n",
    "    qkv_features: dimension of the key, query, and value.\n",
    "    out_features: dimension of the last projection\n",
    "    attention_axis: axes over which the attention is applied ( 'None' means\n",
    "      attention over all axes, but batch, heads, and features).\n",
    "    causal_mask: boolean specifying whether to apply a causal mask on the\n",
    "      attention weights. If True, the output at timestep `t` will not depend\n",
    "      on inputs at timesteps strictly greater than `t`.\n",
    "    padding_mask: boolean specifying query tokens that are pad token.\n",
    "    key_padding_mask: boolean specifying key-value tokens that are pad token.\n",
    "    segmentation: segment indices for packed inputs_q data.\n",
    "    key_segmentation: segment indices for packed inputs_kv data.\n",
    "    cache: an instance of `flax.nn.attention.Cache` used for efficient\n",
    "      autoregressive decoding.\n",
    "    broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n",
    "    dropout_rng: JAX PRNGKey: to be used for dropout\n",
    "    dropout_rate: dropout rate\n",
    "    deterministic: bool, deterministic or not (to apply dropout)\n",
    "    precision: numerical precision of the computation see `jax.lax.Precision`\n",
    "      for details.\n",
    "    kernel_init: initializer for the kernel of the Dense layers.\n",
    "    bias_init: initializer for the bias of the Dense layers.\n",
    "    bias: bool: whether pointwise QKVO dense transforms use bias.\n",
    "    attention_fn: dot_product_attention or compatible function. Accepts\n",
    "    query, key, value, and returns output of shape\n",
    "    `[bs, dim1, dim2, ..., dimN,, num_heads, value_channels]``\n",
    "\n",
    "  Returns:\n",
    "    output of shape `[bs, dim1, dim2, ..., dimN, features]`.\n",
    "  \"\"\"\n",
    "\n",
    "  assert causal_mask or not cache, (\n",
    "      'Caching is only support for causal attention.')\n",
    "\n",
    "  if inputs_kv is None:\n",
    "    inputs_kv = inputs_q\n",
    "\n",
    "  if attention_axis is None:\n",
    "    attention_axis = tuple(range(1, inputs_q.ndim - 1))\n",
    "\n",
    "  features = out_features or inputs_q.shape[-1]\n",
    "  qkv_features = qkv_features or inputs_q.shape[-1]\n",
    "\n",
    "  assert qkv_features % num_heads == 0, (\n",
    "      'Memory dimension must be divisible by number of heads.')\n",
    "  head_dim = qkv_features // num_heads\n",
    "\n",
    "  dense = functools.partial(dense_general,\n",
    "      axis=-1,\n",
    "      dtype=dtype,\n",
    "      features=(num_heads, head_dim),\n",
    "      kernel_init=kernel_init,\n",
    "      bias_init=bias_init,\n",
    "      bias=bias,\n",
    "      precision=precision)\n",
    "  # project inputs_q to multi-headed q/k/v\n",
    "  # dimensions are then [bs, dims..., n_heads, n_features_per_head]\n",
    "  query = scope.child(dense, 'query')(inputs_q)\n",
    "  key = scope.child(dense, 'key')(inputs_kv)\n",
    "  value = scope.child(dense, 'value')(inputs_kv)\n",
    "\n",
    "  if cache:\n",
    "    if not scope.has_variable('cache', 'entry'):\n",
    "      ndim, tail_shape = (key.ndim, key.shape[-2:])\n",
    "      def init_fn(shape):\n",
    "        full_shape = shape + tail_shape\n",
    "        if len(full_shape) != ndim:\n",
    "          raise ValueError('Shape should be a tuple with the shape of the batch'\n",
    "                          'and attention dims.')\n",
    "        return CacheEntry(\n",
    "            key=jnp.zeros(full_shape),\n",
    "            value=jnp.zeros(full_shape),\n",
    "            i=jnp.zeros((), jnp.uint32))\n",
    "      cache_entry = init_fn\n",
    "    else:\n",
    "      cache_entry = scope.get_variable('cache', 'entry')\n",
    "      if not isinstance(cache_entry, CacheEntry):\n",
    "        raise ValueError('Cache is not initialized.')\n",
    "\n",
    "      expected_shape = list(cache_entry.key.shape[:-2])\n",
    "      for attn_dim in attention_axis:\n",
    "        expected_shape[attn_dim] = 1\n",
    "      expected_shape = tuple(expected_shape) + inputs_q.shape[-1:]\n",
    "      if expected_shape != inputs_q.shape:\n",
    "        raise ValueError('Invalid shape provided, '\n",
    "                          'expected shape %s instead got %s.' %\n",
    "                          (expected_shape, inputs_q.shape))\n",
    "\n",
    "      cshape = cache_entry.key.shape\n",
    "      indices = [0] * len(cshape)\n",
    "      i = cache_entry.i\n",
    "      attn_size = onp.prod(onp.take(cshape, attention_axis))\n",
    "      for attn_dim in attention_axis:\n",
    "        attn_size //= cshape[attn_dim]\n",
    "        indices[attn_dim] = i // attn_size\n",
    "        i = i % attn_size\n",
    "\n",
    "      key = lax.dynamic_update_slice(cache_entry.key, key, indices)\n",
    "      value = lax.dynamic_update_slice(cache_entry.value, value, indices)\n",
    "      one = jnp.array(1, jnp.uint32)\n",
    "      cache_entry = cache_entry.replace(i=cache_entry.i + one,\n",
    "                                        key=key,\n",
    "                                        value=value)\n",
    "      \n",
    "\n",
    "      # TODO(levskaya): verify this is still needed in translation decoding.\n",
    "      key_padding_mask = jnp.broadcast_to(\n",
    "          (jnp.arange(cshape[1]) < cache_entry.i), cshape[:2])\n",
    "      key_padding_mask = key_padding_mask.astype(jnp.float32)[..., None]\n",
    "    scope.put_variable('cache', 'entry', cache_entry)\n",
    "\n",
    "  # create attention masks\n",
    "  mask_components = []\n",
    "\n",
    "  if causal_mask:\n",
    "    if cache and isinstance(cache_entry, CacheEntry):\n",
    "      bias_pre_shape = (1,) * (key.ndim - 1)\n",
    "      attn_shape = tuple(onp.take(key.shape, attention_axis))\n",
    "      attn_size = onp.prod(attn_shape)\n",
    "      ii = jnp.arange(attn_size, dtype=jnp.uint32)\n",
    "      mask = ii < cache_entry.i\n",
    "      mask_components.append(mask.reshape(bias_pre_shape + attn_shape))\n",
    "    else:\n",
    "      mask_components.append(nn.attention._make_causal_mask(key, attention_axis))\n",
    "\n",
    "  if padding_mask is not None:\n",
    "    if key_padding_mask is None:\n",
    "      key_padding_mask = padding_mask\n",
    "    padding_mask = nn.attention.make_padding_mask(\n",
    "        padding_mask_query=padding_mask,\n",
    "        padding_mask_key=key_padding_mask,\n",
    "        query_shape=query.shape,\n",
    "        key_shape=key.shape,\n",
    "        attention_axis=attention_axis)\n",
    "    mask_components.append(padding_mask)\n",
    "\n",
    "  if segmentation is not None:\n",
    "    if key_segmentation is None:\n",
    "      key_segmentation = segmentation\n",
    "    segmentation_mask = nn.attention.make_padding_mask(\n",
    "        padding_mask_query=segmentation,\n",
    "        padding_mask_key=key_segmentation,\n",
    "        query_shape=query.shape,\n",
    "        key_shape=key.shape,\n",
    "        attention_axis=attention_axis,\n",
    "        segmentation_mask=True)\n",
    "    mask_components.append(segmentation_mask)\n",
    "\n",
    "  if mask_components:\n",
    "    attention_mask = mask_components[0]\n",
    "    for component in mask_components[1:]:\n",
    "      attention_mask = jnp.logical_and(attention_mask, component)\n",
    "\n",
    "    # attention mask in the form of attention bias\n",
    "    attention_bias = lax.select(\n",
    "        attention_mask > 0, jnp.full(attention_mask.shape, 0.).astype(dtype),\n",
    "        jnp.full(attention_mask.shape, -1e10).astype(dtype))\n",
    "  else:\n",
    "    attention_bias = None\n",
    "\n",
    "  # apply attention\n",
    "  x = attention_fn(\n",
    "      query,\n",
    "      key,\n",
    "      value,\n",
    "      dtype=dtype,\n",
    "      axis=attention_axis,\n",
    "      bias=attention_bias,\n",
    "      precision=precision,\n",
    "      dropout_rng=dropout_rng,\n",
    "      dropout_rate=dropout_rate,\n",
    "      broadcast_dropout=broadcast_dropout,\n",
    "      deterministic=deterministic)\n",
    "\n",
    "  # back to the original inputs dimensions\n",
    "  out = scope.child(dense_general, name='out')(\n",
    "      x,\n",
    "      features=features,\n",
    "      axis=(-2, -1),\n",
    "      kernel_init=kernel_init,\n",
    "      bias_init=bias_init,\n",
    "      bias=bias,\n",
    "      dtype=dtype,\n",
    "      precision=precision)\n",
    "\n",
    "  return out\n",
    "\n",
    "x = jnp.ones((1, 2, 4))\n",
    "attn = functools.partial(multi_head_dot_product_attention, num_heads=1, cache=True, causal_mask=True)\n",
    "y, variables = init(attn)(random.PRNGKey(0), x, x)\n",
    "params = variables['param']\n",
    "cache = jax.tree_map(lambda fn: fn((1, 2)), variables['cache'])\n",
    "variables = variables.copy(cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1590672864921,
     "user": {
      "displayName": "Jonathan Heek",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
      "userId": "00491914421152177709"
     },
     "user_tz": -120
    },
    "id": "xd3Smw99EWjC",
    "outputId": "f010a3c2-f07d-4ef0-f1b0-21d44877ba05"
   },
   "outputs": [],
   "source": [
    "apply(attn, mutable='cache')(variables, x[:, 0:1], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1590672865722,
     "user": {
      "displayName": "Jonathan Heek",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
      "userId": "00491914421152177709"
     },
     "user_tz": -120
    },
    "id": "LTFjZbRmlqZh",
    "outputId": "5790b763-df4f-47c8-9f4e-53fd1e1eb1fd"
   },
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Embedding:\n",
    "  table: onp.ndarray\n",
    "\n",
    "  def lookup(self, indices):\n",
    "    return self.table[indices]\n",
    "\n",
    "  def attend(self, query):\n",
    "    return jnp.dot(query, self.table.T)\n",
    "\n",
    "# all the embedding module does is provide a convenient initializers\n",
    "\n",
    "def embedding(scope: Scope, num_embeddings: int, features: int, init_fn=nn.linear.default_embed_init) -> Embedding:\n",
    "  table = scope.param('table', init_fn, (num_embeddings, features))\n",
    "  return Embedding(table)\n",
    "\n",
    "embedding, _ = init(embedding)(random.PRNGKey(0), num_embeddings=2, features=3)\n",
    "print(embedding.table)\n",
    "print(embedding.lookup(1))\n",
    "print(embedding.attend(jnp.ones((1, 3,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1590673618925,
     "user": {
      "displayName": "Jonathan Heek",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
      "userId": "00491914421152177709"
     },
     "user_tz": -120
    },
    "id": "TMlae0hem0u5",
    "outputId": "dd9c5079-10e7-4944-e09a-e9f65573a733"
   },
   "outputs": [],
   "source": [
    "def lstm(scope, carry, inputs,\n",
    "         gate_fn=nn.activation.sigmoid, activation_fn=nn.activation.tanh,\n",
    "         kernel_init=nn.linear.default_kernel_init,\n",
    "         recurrent_kernel_init=nn.initializers.orthogonal(),\n",
    "         bias_init=nn.initializers.zeros):\n",
    "  r\"\"\"A long short-term memory (LSTM) cell.\n",
    "\n",
    "  the mathematical definition of the cell is as follows\n",
    "  .. math::\n",
    "      \\begin{array}{ll}\n",
    "      i = \\sigma(W_{ii} x + W_{hi} h + b_{hi}) \\\\\n",
    "      f = \\sigma(W_{if} x + W_{hf} h + b_{hf}) \\\\\n",
    "      g = \\tanh(W_{ig} x + W_{hg} h + b_{hg}) \\\\\n",
    "      o = \\sigma(W_{io} x + W_{ho} h + b_{ho}) \\\\\n",
    "      c' = f * c + i * g \\\\\n",
    "      h' = o * \\tanh(c') \\\\\n",
    "      \\end{array}\n",
    "  where x is the input, h is the output of the previous time step, and c is\n",
    "  the memory.\n",
    "\n",
    "  Args:\n",
    "    carry: the hidden state of the LSTM cell,\n",
    "      initialized using `LSTMCell.initialize_carry`.\n",
    "    inputs: an ndarray with the input for the current time step.\n",
    "      All dimensions except the final are considered batch dimensions.\n",
    "    gate_fn: activation function used for gates (default: sigmoid)\n",
    "    activation_fn: activation function used for output and memory update\n",
    "      (default: tanh).\n",
    "    kernel_init: initializer function for the kernels that transform\n",
    "      the input (default: lecun_normal).\n",
    "    recurrent_kernel_init: initializer function for the kernels that transform\n",
    "      the hidden state (default: orthogonal).\n",
    "    bias_init: initializer for the bias parameters (default: zeros)\n",
    "  Returns:\n",
    "    A tuple with the new carry and the output.\n",
    "  \"\"\"\n",
    "  c, h = carry\n",
    "  hidden_features = h.shape[-1]\n",
    "  # input and recurrent layers are summed so only one needs a bias.\n",
    "  dense_h = lambda name: scope.child(dense, name)(\n",
    "      h, features=hidden_features, bias=True,\n",
    "      kernel_init=recurrent_kernel_init, bias_init=bias_init)\n",
    "  dense_i = lambda name: scope.child(dense, name)(\n",
    "      inputs, features=hidden_features, bias=False,\n",
    "      kernel_init=kernel_init)\n",
    "  i = gate_fn(dense_i(name='ii') + dense_h(name='hi'))\n",
    "  f = gate_fn(dense_i(name='if') + dense_h(name='hf'))\n",
    "  g = activation_fn(dense_i(name='ig') + dense_h(name='hg'))\n",
    "  o = gate_fn(dense_i(name='io') + dense_h(name='ho'))\n",
    "  new_c = f * c + i * g\n",
    "  new_h = o * activation_fn(new_c)\n",
    "  return (new_c, new_h), new_h\n",
    "\n",
    "def lstm_init_carry(batch_dims, size, init_fn=jnp.zeros):\n",
    "  shape = batch_dims + (size,)\n",
    "  return init_fn(shape), init_fn(shape)\n",
    "\n",
    "x = jnp.ones((1, 2))\n",
    "carry = lstm_init_carry((1,), 3)\n",
    "y, variables = init(lstm)(random.PRNGKey(0), carry, x)\n",
    "jax.tree_map(onp.shape, (y, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "flax functional engine.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

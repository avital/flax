{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0.], dtype=float32),\n",
       " 'kernel': DeviceArray([[ 0.20814848, -1.0710016 ,  0.6439947 , -0.5429579 ,\n",
       "               -0.3246118 ,  1.208985  , -0.257471  , -0.32065013,\n",
       "                0.72582203,  0.08994231,  0.3341397 , -0.22079287,\n",
       "                0.69877183,  0.8303373 ,  0.3428869 ,  0.94781363,\n",
       "                0.08524317, -0.45154864,  0.17062634,  0.8318948 ,\n",
       "               -0.6483707 , -0.24204564, -0.04201688, -0.3538871 ,\n",
       "               -0.22361928, -0.07344878, -0.20832072,  0.17268421,\n",
       "               -0.64753693,  0.59740335, -0.07335723, -0.76099616],\n",
       "              [-0.4295275 ,  1.2338059 ,  0.102417  , -0.13127546,\n",
       "               -0.37081522,  0.18604888,  0.19740908, -0.3535717 ,\n",
       "                0.7684898 ,  0.21345675,  0.5770429 , -0.328339  ,\n",
       "                0.04007532,  0.15846986, -0.9928673 ,  0.2212549 ,\n",
       "               -0.3701994 , -0.6424554 , -0.3648998 ,  0.12597178,\n",
       "               -0.02469591, -0.27812898, -0.22645561, -0.09129761,\n",
       "                0.7858967 ,  1.0210173 , -0.27159536, -0.64969337,\n",
       "                0.10275015, -0.40656534, -0.5714007 , -0.10323345],\n",
       "              [-0.33381036,  0.8070612 ,  0.21314   ,  0.499298  ,\n",
       "               -0.2794949 ,  0.5282369 ,  0.16104864, -0.26513886,\n",
       "               -0.19287321, -0.19362137,  0.13071   , -0.40732282,\n",
       "                0.9995863 ,  0.54519784,  0.12906857,  1.0335478 ,\n",
       "               -0.4564211 , -0.49224326, -0.30296874,  0.37264237,\n",
       "               -0.15083975, -0.8947372 ,  0.22655495, -0.61037207,\n",
       "               -0.06121872,  0.7279354 ,  0.66585314, -0.8731882 ,\n",
       "                0.27885145,  0.5800933 , -0.04471758,  0.32257393]],            dtype=float32)}"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Dense(Module):\n",
    "  features: int\n",
    "  bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "\n",
    "  def __call__(self, x):\n",
    "    kernel = self.param('kernel', self.kernel_init, (x.shape[-1], self.features))\n",
    "    x = jnp.dot(x, kernel)\n",
    "    if self.bias:\n",
    "      x = x + self.param('bias', self.bias_init, (self.features,))\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DenseExplicit(Module):\n",
    "  in_features: int\n",
    "  out_features: int\n",
    "  bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "\n",
    "  def __post_init__(self):\n",
    "    self.param('kernel', self.kernel_init, (self.in_features, self.out_features))\n",
    "    if self.bias:\n",
    "      self.param('bias', self.bias_init, (self.out_features,))\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    x = jnp.dot(x, self.param('kernel'))\n",
    "    if self.bias:\n",
    "      x = x + self.param('bias')\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class DenseExplicit2(Module):\n",
    "  in_features: int\n",
    "  out_features: int\n",
    "  bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "\n",
    "  def init(self):\n",
    "    # NOTE: Can't do this in __init__ because we don't have a scope yet hence no prngs\n",
    "    self.kernel = self.param('kernel', self.kernel_init, (self.in_features, self.out_features))\n",
    "    if self.bias:\n",
    "      self.bias_ = self.param('bias', self.bias_init, (self.out_features,))\n",
    "    return self\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not hasattr(self, 'kernel'):\n",
    "      raise ValueError(\"Must call `init` first.\")\n",
    "    x = jnp.dot(x, self.kernel)\n",
    "    if self.bias:\n",
    "      x = x + self.bias_\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLP1(Module):\n",
    "  depth: int = 3\n",
    "  width: int = 32\n",
    "  features: int = 10\n",
    "\n",
    "  def call1(self, x):\n",
    "    # TODO: catch error if you do [Dense(self.width)] * 3?\n",
    "    # identify isntances based on reference equality?\n",
    "    #\n",
    "    # Or alternatively, is it cheap enough to clone each item in the list?\n",
    "    layers = self.module_list('layers', [Dense(self.width)] * self.depth)\n",
    "    # QUESTION: Is this necessary?\n",
    "    layers = self.module_list('layers', [Dense(self.width) for _ in range(self.depth)])\n",
    "    for layer in layers:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.child('final', Dense(self.features))(x)\n",
    "    return x\n",
    "\n",
    "  def call2(self, x):\n",
    "    # TODO: catch error if you do [Dense(self.width)] * 3?\n",
    "    # identify isntances based on reference equality?\n",
    "    intermediate = self.module_list('intermediate')\n",
    "    for i in range(self.depth):\n",
    "      x = nn.relu(intermediate.child(Dense(self.width)))\n",
    "    x = self.child('final', Dense(self.features))(x)\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class MLP2(Module):\n",
    "  depth: int = 3\n",
    "  width: int = 32\n",
    "  features: int = 10\n",
    "\n",
    "  # QUESTION: Can we do doubly nested module lists?\n",
    "    \n",
    "  def __post_init__(self):\n",
    "    self.intermediate = self.module_list('intermediate', [Dense(self.width)] * self.depth)\n",
    "    # TODO: Maybe make an error if you call `child` on a module list assigned to an instance?\n",
    "    # Otherwise cursor logic doesn't let you call the same method twice\n",
    "\n",
    "  def call(self, x):\n",
    "    for layer in self.intermediate:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.add('final', Dense(self.width))(x)\n",
    "    return x\n",
    "  \n",
    "  def call2(self, x):\n",
    "    for layer in self.intermediate:\n",
    "      x = nn.relu(layer(x))\n",
    "    # NOTE: This should fail, I think it can't be made to work correctly.\n",
    "    # (sharing a ModuleList across methods)\n",
    "    x = self.layers.add(Dense(self.width))(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class MLP3(Module):\n",
    "  depth: int = 3\n",
    "  width: int = 32\n",
    "  features: int = 10\n",
    "\n",
    "  def __post_init__(self):\n",
    "    self.intermediate = self.module_list('intermediate', [Dense(self.width)] * self.depth)\n",
    "    \n",
    "    # TODO: Maybe make an error if you call `child` on a module list assigned to an instance?\n",
    "    # Otherwise cursor logic doesn't let you call the same method twice\n",
    "\n",
    "  def call(self, x):\n",
    "    for layer in self.intermediate:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.add('final', Dense(self.width))(x)\n",
    "    return x\n",
    "  \n",
    "  def call2(self, x):\n",
    "    for layer in self.intermediate:\n",
    "      x = nn.relu(layer(x))\n",
    "    # NOTE: This should fail, I think it can't be made to work correctly.\n",
    "    # (sharing a ModuleList across methods)\n",
    "    x = self.intermediate.add(Dense(self.width))(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class MLP4(Module):\n",
    "  depth: int = 3\n",
    "  width: int = 32\n",
    "  in_features: int = 28*28\n",
    "  out_features: int = 10\n",
    "    \n",
    "  def __post_init__(self):\n",
    "    # ASK JONATHAN(!) If we create a scope lazily, and then say: when you make something a child,\n",
    "    # use that scope if it has one, otherwise ...\n",
    "    #\n",
    "    # Wait, but how do we do that? Need to un-transpose the scope with parameters back to the top level.\n",
    "    #\n",
    "    # I think this is the argument for making scope own its variables rather than look at parent.\n",
    "    #\n",
    "    # NOTE!!! This really won't work because there's no scope therefore no prng available\n",
    "    # NOTE: This doesn't work; how does DenseExplicit know the prng?\n",
    "    self.initial = self.submodule('initial', DenseExplicit(self.in_features, self.width))\n",
    "    # TODO: Make `self.submodule` call `.init()` on any modules that have it.\n",
    "    self.intermediate = self.module_list('intermediate', [DenseExplicit(self.width, self.width)] * (self.depth-1))\n",
    "    self.final = self.submodule('final', DenseExplicit(self.width, self.out_features))\n",
    "\n",
    "  def init(self):\n",
    "    self.initial = self.submodule('initial', DenseExplicit(self.in_features, self.width))\n",
    "    self.intermediate = self.module_list('intermediate', [DenseExplicit(self.width, self.width)] * (self.depth-1))\n",
    "    self.final = self.submodule('final', DenseExplicit(self.width, self.out_features))\n",
    "    return self\n",
    "    \n",
    "    \n",
    "  def call(self, x):\n",
    "    self.init()\n",
    "    x = self.initial(x)\n",
    "    for layer in self.intermediates:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.final(x)\n",
    "    return x\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "@dataclass\n",
    "class MLP5(Module):\n",
    "  depth: int = 3\n",
    "  width: int = 32\n",
    "  features: int = 10\n",
    "\n",
    "  def __call__(self, x):\n",
    "    intermediate = self.module_list('intermediate', [Dense(self.features)] * self.depth)\n",
    "    for layer in intermediate:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.submodule('final', Dense(self.width))(x)\n",
    "    return x\n",
    "\n",
    "  \n",
    "\n",
    "@dataclass\n",
    "class MLP6(Module):\n",
    "  depth: int = 3\n",
    "  width: int = 32\n",
    "  features: int = 10\n",
    "\n",
    "  def __call__(self, x):\n",
    "    layers = self.module_list('layers', ([Dense(self.width)] * self.depth) + [Dense(self.features)])\n",
    "    for layer in layers:\n",
    "      x = nn.relu(layer(x))\n",
    "    return x\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "@jit\n",
    "def init():\n",
    "  x = np.ones((3, 3))\n",
    "  mlp = MLP(depth=3, width=32)\n",
    "  mlp(x)\n",
    "  return mlp\n",
    "\n",
    "module = init()\n",
    "dir(module)\n",
    "# TODO: Consider making `module.layers` more of an actual list by extending it?\n",
    "module.layers.children[0].params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0903456 [[-0.04370475 -0.14652367 -0.20754965  0.03639566]]\n",
      "1.0903456 [[ 0.00516562 -0.04317067 -0.07185964  0.04282168]]\n",
      "1.0167608 [[0.05 0.05 0.05 0.05]]\n",
      "0.95 [[0.075 0.075 0.075 0.075]]\n",
      "0.925 [[0.1 0.1 0.1 0.1]]\n",
      "0.9 [[0.125 0.125 0.125 0.125]]\n",
      "0.875 [[0.15 0.15 0.15 0.15]]\n",
      "0.85 [[0.17500001 0.17500001 0.17500001 0.17500001]]\n",
      "0.825 [[0.20000002 0.20000002 0.20000002 0.20000002]]\n",
      "0.79999995 [[0.22500002 0.22500002 0.22500002 0.22500002]]\n",
      "0.775 [[0.25000003 0.25000003 0.25000003 0.25000003]]\n",
      "0.75 [[0.27500004 0.27500004 0.27500004 0.27500004]]\n",
      "0.72499996 [[0.30000004 0.30000004 0.30000004 0.30000004]]\n",
      "0.6999999 [[0.32500005 0.32500005 0.32500005 0.32500005]]\n",
      "0.67499995 [[0.35000005 0.35000005 0.35000005 0.35000005]]\n",
      "0.65 [[0.37500006 0.37500006 0.37500006 0.37500006]]\n",
      "0.62499994 [[0.40000007 0.40000007 0.40000007 0.40000007]]\n",
      "0.5999999 [[0.42500007 0.42500007 0.42500007 0.42500007]]\n",
      "0.5749999 [[0.45000008 0.45000008 0.45000008 0.45000008]]\n",
      "0.54999995 [[0.47500008 0.47500008 0.47500008 0.47500008]]\n",
      "0.5249999 [[0.50000006 0.50000006 0.50000006 0.50000006]]\n",
      "0.49999994 [[0.52500004 0.52500004 0.52500004 0.52500004]]\n",
      "0.47499996 [[0.55 0.55 0.55 0.55]]\n",
      "0.45 [[0.575 0.575 0.575 0.575]]\n",
      "0.425 [[0.59999996 0.59999996 0.59999996 0.59999996]]\n",
      "0.40000004 [[0.62499994 0.62499994 0.62499994 0.62499994]]\n",
      "0.37500006 [[0.6499999 0.6499999 0.6499999 0.6499999]]\n",
      "0.35000008 [[0.6749999 0.6749999 0.6749999 0.6749999]]\n",
      "0.3250001 [[0.69999987 0.69999987 0.69999987 0.69999987]]\n",
      "0.30000013 [[0.72499985 0.72499985 0.72499985 0.72499985]]\n",
      "0.27500015 [[0.7499998 0.7499998 0.7499998 0.7499998]]\n",
      "0.25000018 [[0.7749998 0.7749998 0.7749998 0.7749998]]\n",
      "0.2250002 [[0.7999998 0.7999998 0.7999998 0.7999998]]\n",
      "0.20000023 [[0.82499975 0.82499975 0.82499975 0.82499975]]\n",
      "0.17500025 [[0.8499997 0.8499997 0.8499997 0.8499997]]\n",
      "0.15000027 [[0.8749997 0.8749997 0.8749997 0.8749997]]\n",
      "0.1250003 [[0.8999997 0.8999997 0.8999997 0.8999997]]\n",
      "0.10000032 [[0.92499965 0.92499965 0.92499965 0.92499965]]\n",
      "0.075000346 [[0.94999963 0.94999963 0.94999963 0.94999963]]\n",
      "0.05000037 [[0.9749996 0.9749996 0.9749996 0.9749996]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTE: This is again like Flax's current `Model` -- you take gradients\n",
    "# w.r.t. it.\n",
    "def loss_fn(mlp):\n",
    "  x = np.ones((1, 3))\n",
    "  y_true = np.ones((1, 4))\n",
    "  y_pred = mlp(x)\n",
    "  return jnp.mean(jnp.abs(y_pred - y_true))\n",
    "\n",
    "@jit\n",
    "def opt_step(mlp):\n",
    "  # TODO: Mark `mlp` as dead\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(mlp)\n",
    "  lr = 1e-1\n",
    "  return loss, jax.tree_multimap(lambda w, g: w - lr * g, mlp, grad)\n",
    "\n",
    "mlp = MLP(depth=3, width=4)\n",
    "for i in range(40):\n",
    "  loss, mlp = opt_step(mlp)\n",
    "  print(loss, mlp(np.ones((1, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(value=DeviceArray(3, dtype=int32))\n",
      "Counter(value=DeviceArray(6, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "@dataclass\n",
    "class Counter:\n",
    "  # QUESTION: register_buffer_variable? field(kind='param'|'counter'|'log', ...)\n",
    "  # Need special wrapper that's not Module. More like Jonathan's dataclass. Then\n",
    "  # Have Module use that decorator as well.\n",
    "  value: int = 0\n",
    "\n",
    "  def __call__(self):\n",
    "    self.value = self.value + 1\n",
    "\n",
    "  def tree_flatten(self):\n",
    "    return (self.value, ), None\n",
    "  \n",
    "  @classmethod\n",
    "  def tree_unflatten(cls, meta, data):\n",
    "    value, = data\n",
    "    return cls(value)\n",
    "\n",
    "counter = Counter()\n",
    "@jit\n",
    "def inc3(c):\n",
    "  c()\n",
    "  c()\n",
    "  c()\n",
    "  return c\n",
    "\n",
    "counter = inc3(counter)\n",
    "print(counter)\n",
    "counter = inc3(counter)\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(value=DeviceArray(3, dtype=int32))\n",
      "Counter(value=DeviceArray(6, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "@tree_util.register_pytree_node_class\n",
    "class WithCounter(Module):\n",
    "  def __call__(self):\n",
    "    # QUESTION: Use __setattr__ so that this becomes\n",
    "    #     self.counter = Counter()?\n",
    "    self.counter = self.child(\"counter\", Counter())\n",
    "    pass\n",
    "\n",
    "# TODO: @cloned -- simulate jit but still being able to debug\n",
    "  \n",
    "@jit\n",
    "def inc3_with(with_counter):\n",
    "  with_counter.counter()\n",
    "  with_counter.counter()\n",
    "  with_counter.counter()\n",
    "  return with_counter\n",
    "\n",
    "with_counter = WithCounter()\n",
    "with_counter()\n",
    "# TODO: Somehow make this code fail -- it doesn't behave\n",
    "# the same under a jit\n",
    "# print(increment_twice(with_counter).counter)\n",
    "# print(increment_twice(with_counter).counter)\n",
    "\n",
    "with_counter = inc3_with(with_counter)\n",
    "print(with_counter.counter)\n",
    "with_counter = inc3_with(with_counter)\n",
    "print(with_counter.counter)\n",
    "\n",
    "# TODO: If we use __setattr__ then can we not place those things on `self.params`? Then\n",
    "# this won't be possible to try.\n",
    "# with_counter = inc3_with(with_counter)\n",
    "# print(with_counter.params['counter'])\n",
    "# with_counter = inc3_with(with_counter)\n",
    "# print(with_counter.params['counter'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(depth=3, width=4, features=10)\n",
      "1.0903456 Counter(value=1) [[ 0.00516562 -0.04317067 -0.07185964  0.04282168]]\n",
      "1.0167607 Counter(value=2) [[0.05 0.05 0.05 0.05]]\n",
      "0.95 Counter(value=3) [[0.075 0.075 0.075 0.075]]\n",
      "0.925 Counter(value=4) [[0.1 0.1 0.1 0.1]]\n",
      "0.9 Counter(value=5) [[0.125 0.125 0.125 0.125]]\n",
      "0.875 Counter(value=6) [[0.15 0.15 0.15 0.15]]\n",
      "0.85 Counter(value=7) [[0.17500001 0.17500001 0.17500001 0.17500001]]\n",
      "0.825 Counter(value=8) [[0.20000002 0.20000002 0.20000002 0.20000002]]\n",
      "0.79999995 Counter(value=9) [[0.22500002 0.22500002 0.22500002 0.22500002]]\n",
      "0.775 Counter(value=10) [[0.25000003 0.25000003 0.25000003 0.25000003]]\n",
      "0.75 Counter(value=11) [[0.27500004 0.27500004 0.27500004 0.27500004]]\n",
      "0.72499996 Counter(value=12) [[0.30000004 0.30000004 0.30000004 0.30000004]]\n",
      "0.6999999 Counter(value=13) [[0.32500005 0.32500005 0.32500005 0.32500005]]\n",
      "0.67499995 Counter(value=14) [[0.35000005 0.35000005 0.35000005 0.35000005]]\n",
      "0.65 Counter(value=15) [[0.37500006 0.37500006 0.37500006 0.37500006]]\n",
      "0.62499994 Counter(value=16) [[0.40000007 0.40000007 0.40000007 0.40000007]]\n",
      "0.5999999 Counter(value=17) [[0.42500007 0.42500007 0.42500007 0.42500007]]\n",
      "0.5749999 Counter(value=18) [[0.45000008 0.45000008 0.45000008 0.45000008]]\n",
      "0.54999995 Counter(value=19) [[0.47500008 0.47500008 0.47500008 0.47500008]]\n",
      "0.5249999 Counter(value=20) [[0.50000006 0.50000006 0.50000006 0.50000006]]\n",
      "0.49999994 Counter(value=21) [[0.52500004 0.52500004 0.52500004 0.52500004]]\n",
      "0.47499996 Counter(value=22) [[0.55 0.55 0.55 0.55]]\n",
      "0.45 Counter(value=23) [[0.575 0.575 0.575 0.575]]\n",
      "0.425 Counter(value=24) [[0.59999996 0.59999996 0.59999996 0.59999996]]\n",
      "0.40000004 Counter(value=25) [[0.62499994 0.62499994 0.62499994 0.62499994]]\n",
      "0.37500006 Counter(value=26) [[0.6499999 0.6499999 0.6499999 0.6499999]]\n",
      "0.35000008 Counter(value=27) [[0.6749999 0.6749999 0.6749999 0.6749999]]\n",
      "0.3250001 Counter(value=28) [[0.69999987 0.69999987 0.69999987 0.69999987]]\n",
      "0.30000013 Counter(value=29) [[0.72499985 0.72499985 0.72499985 0.72499985]]\n",
      "0.27500015 Counter(value=30) [[0.7499998 0.7499998 0.7499998 0.7499998]]\n",
      "0.25000018 Counter(value=31) [[0.7749998 0.7749998 0.7749998 0.7749998]]\n",
      "0.2250002 Counter(value=32) [[0.7999998 0.7999998 0.7999998 0.7999998]]\n",
      "0.20000023 Counter(value=33) [[0.82499975 0.82499975 0.82499975 0.82499975]]\n",
      "0.17500025 Counter(value=34) [[0.8499997 0.8499997 0.8499997 0.8499997]]\n",
      "0.15000027 Counter(value=35) [[0.8749997 0.8749997 0.8749997 0.8749997]]\n",
      "0.1250003 Counter(value=36) [[0.8999997 0.8999997 0.8999997 0.8999997]]\n",
      "0.10000032 Counter(value=37) [[0.92499965 0.92499965 0.92499965 0.92499965]]\n",
      "0.075000346 Counter(value=38) [[0.94999963 0.94999963 0.94999963 0.94999963]]\n",
      "0.05000037 Counter(value=39) [[0.9749996 0.9749996 0.9749996 0.9749996]]\n",
      "0.025000393 Counter(value=40) [[0.9999996 0.9999996 0.9999996 0.9999996]]\n"
     ]
    }
   ],
   "source": [
    "@tree_util.register_pytree_node_class\n",
    "class MLPAndCounter(Module):\n",
    "  def __init__(self):\n",
    "    self.mlp = self.child('mlp', MLP(depth=3, width=4))\n",
    "    self.counter = self.child('counter', Counter())\n",
    "\n",
    "def clone(x):\n",
    "  return jax.tree_map(lambda v: v, x)\n",
    "    \n",
    "# TODO: Is there a general pattern for extracting just the \n",
    "# trainable parameters? Use `kind`\n",
    "def opt_step(mlp_and_counter):\n",
    "  mlp_and_counter = clone(mlp_and_counter)\n",
    "  mlp_and_counter.counter()\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(mlp_and_counter.mlp)\n",
    "  lr = 1e-1\n",
    "  # TODO: This really implies we should probably override __setattr__\n",
    "  # TODO(!!!): Why did I need to do this? Mental model breakdown!\n",
    "  old_mlp = mlp_and_counter.mlp\n",
    "  del mlp_and_counter.params['mlp']\n",
    "  del mlp_and_counter.mlp\n",
    "  mlp_and_counter.mlp = mlp_and_counter.child('mlp', jax.tree_multimap(lambda w, g: w - lr * g, old_mlp, grad))\n",
    "  return loss, mlp_and_counter\n",
    "\n",
    "@jit\n",
    "def init(): \n",
    "  mlp_and_counter = MLPAndCounter()\n",
    "  mlp_and_counter.mlp(np.ones((1, 3)))\n",
    "  print(mlp_and_counter.mlp)\n",
    "  return mlp_and_counter\n",
    "\n",
    "mlp_and_counter = init()\n",
    "\n",
    "for i in range(40):\n",
    "  loss, mlp_and_counter = opt_step(mlp_and_counter)\n",
    "  print(loss, mlp_and_counter.counter, mlp_and_counter.mlp(np.ones((1, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(depth=3, width=4, features=10)\n",
      "1.0903456 2 [[ 0.00516562 -0.04317067 -0.07185964  0.04282168]]\n",
      "1.0167607 3 [[0.05 0.05 0.05 0.05]]\n",
      "0.95 4 [[0.075 0.075 0.075 0.075]]\n",
      "0.925 5 [[0.1 0.1 0.1 0.1]]\n",
      "0.9 6 [[0.125 0.125 0.125 0.125]]\n",
      "0.875 7 [[0.15 0.15 0.15 0.15]]\n",
      "0.85 8 [[0.17500001 0.17500001 0.17500001 0.17500001]]\n",
      "0.825 9 [[0.20000002 0.20000002 0.20000002 0.20000002]]\n",
      "0.79999995 10 [[0.22500002 0.22500002 0.22500002 0.22500002]]\n",
      "0.775 11 [[0.25000003 0.25000003 0.25000003 0.25000003]]\n",
      "0.75 12 [[0.27500004 0.27500004 0.27500004 0.27500004]]\n",
      "0.72499996 13 [[0.30000004 0.30000004 0.30000004 0.30000004]]\n",
      "0.6999999 14 [[0.32500005 0.32500005 0.32500005 0.32500005]]\n",
      "0.67499995 15 [[0.35000005 0.35000005 0.35000005 0.35000005]]\n",
      "0.65 16 [[0.37500006 0.37500006 0.37500006 0.37500006]]\n",
      "0.62499994 17 [[0.40000007 0.40000007 0.40000007 0.40000007]]\n",
      "0.5999999 18 [[0.42500007 0.42500007 0.42500007 0.42500007]]\n",
      "0.5749999 19 [[0.45000008 0.45000008 0.45000008 0.45000008]]\n",
      "0.54999995 20 [[0.47500008 0.47500008 0.47500008 0.47500008]]\n",
      "0.5249999 21 [[0.50000006 0.50000006 0.50000006 0.50000006]]\n",
      "0.49999994 22 [[0.52500004 0.52500004 0.52500004 0.52500004]]\n",
      "0.47499996 23 [[0.55 0.55 0.55 0.55]]\n",
      "0.45 24 [[0.575 0.575 0.575 0.575]]\n",
      "0.425 25 [[0.59999996 0.59999996 0.59999996 0.59999996]]\n",
      "0.40000004 26 [[0.62499994 0.62499994 0.62499994 0.62499994]]\n",
      "0.37500006 27 [[0.6499999 0.6499999 0.6499999 0.6499999]]\n",
      "0.35000008 28 [[0.6749999 0.6749999 0.6749999 0.6749999]]\n",
      "0.3250001 29 [[0.69999987 0.69999987 0.69999987 0.69999987]]\n",
      "0.30000013 30 [[0.72499985 0.72499985 0.72499985 0.72499985]]\n",
      "0.27500015 31 [[0.7499998 0.7499998 0.7499998 0.7499998]]\n",
      "0.25000018 32 [[0.7749998 0.7749998 0.7749998 0.7749998]]\n",
      "0.2250002 33 [[0.7999998 0.7999998 0.7999998 0.7999998]]\n",
      "0.20000023 34 [[0.82499975 0.82499975 0.82499975 0.82499975]]\n",
      "0.17500025 35 [[0.8499997 0.8499997 0.8499997 0.8499997]]\n",
      "0.15000027 36 [[0.8749997 0.8749997 0.8749997 0.8749997]]\n",
      "0.1250003 37 [[0.8999997 0.8999997 0.8999997 0.8999997]]\n",
      "0.10000032 38 [[0.92499965 0.92499965 0.92499965 0.92499965]]\n",
      "0.075000346 39 [[0.94999963 0.94999963 0.94999963 0.94999963]]\n",
      "0.05000037 40 [[0.9749996 0.9749996 0.9749996 0.9749996]]\n",
      "0.025000393 41 [[0.9999996 0.9999996 0.9999996 0.9999996]]\n"
     ]
    }
   ],
   "source": [
    "@tree_util.register_pytree_node_class\n",
    "class MLPAndCounter2(Module):\n",
    "  def __call__(self, x):\n",
    "    self.counter = self.child('counter', Counter())\n",
    "    self.counter()\n",
    "    self.mlp = self.child('mlp', MLP(depth=3, width=4))\n",
    "    return self.mlp(x)\n",
    "  \n",
    "def init2(): \n",
    "  mlp_and_counter = MLPAndCounter2()\n",
    "  mlp_and_counter(np.ones((1, 3)))\n",
    "  print(mlp_and_counter.mlp)\n",
    "  return mlp_and_counter\n",
    "\n",
    "mlp_and_counter2 = init2()\n",
    "\n",
    "for i in range(40):\n",
    "  loss, mlp_and_counter2 = opt_step(mlp_and_counter2)\n",
    "  print(loss, mlp_and_counter2.counter.value, mlp_and_counter2.mlp(np.ones((1, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(value=2)\n",
      "Counter(value=4)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.register_pytree_node_class\n",
    "@dataclass\n",
    "class AutoEncoder(Module):\n",
    "  width: int = 32\n",
    "  depth: int = 3\n",
    "\n",
    "  def encode(self, x):\n",
    "    self.input_shape = x.shape[1:]\n",
    "    \n",
    "    self.encoder = self.module_list('encoder')\n",
    "\n",
    "    # flatten other than batch dimension\n",
    "    B = x.shape[0]\n",
    "    z = np.reshape(x, (B, np.prod(self.input_shape)))\n",
    "\n",
    "    for i in range(self.depth):\n",
    "      z = self.encoder_layers.child(Dense(self.width))(z)\n",
    "      z = nn.relu(z)\n",
    "    # final layer without relu\n",
    "    z = self.encoder_layers.child(Dense(self.width))(z)\n",
    "    return z\n",
    "      \n",
    "  def decode(self, z):\n",
    "    assert hasattr(self, 'input_shape'), \"Need to call `encode` to know the input shape\"\n",
    "    self.decoder_layers = self.module_list('decoder')\n",
    "\n",
    "    x = z\n",
    "    for i in range(self.depth):\n",
    "      x = self.decoder_layers.child(Dense(self.width))(x)\n",
    "      x = nn.relu(x)\n",
    "\n",
    "    x = self.decoder_layers.child(Dense(np.prod(self.input_shape)))(x)\n",
    "    B = x.shape[0]\n",
    "    x = x.reshape((B, ) + self.input_shape)\n",
    "    return x\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dense(5)\n",
    "d.prng_key = jax.random.PRNGKey(0)\n",
    "d(np.ones((10, 10)))\n",
    "\n",
    "# should be 2; weight and bias.\n",
    "len(jax.tree_leaves(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp.prng_key = jax.random.PRNGKey(0)\n",
    "mlp(np.ones((10, 10)))\n",
    "\n",
    "# should be 8 (depth is # of intermediate layer -- so total 4 layers) * (weight and bias).\n",
    "len(jax.tree_leaves(mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae = AutoEncoder()\n",
    "ae(np.ones((10, 32, 32, 3))).shape\n",
    "# should be 16 (encoder, decoder) x (4 total layers (3 intermediate + final)) x (weight, bias)\n",
    "len(jax.tree_leaves(ae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(Module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
